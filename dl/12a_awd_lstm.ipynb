{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWD-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_12 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ubuntu/learnai/dl/data/imdb')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(imdb_path); path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "il = TextList.from_files(path, include=['train', 'test', 'unsup'])\n",
    "sd = SplitData.split_by_func(il, partial(random_splitter, p_valid=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_tok, proc_num = TokenizeProcessor(max_workers=8), NumericalizeProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='45' class='' max='45', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [45/45 01:46<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='6' class='' max='6', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [6/6 00:13<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ll = label_by_func(sd, lambda x:0, proc_x=[proc_tok, proc_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ll, open(path/'ll_lm.pkl', 'wb'))\n",
    "pickle.dump(proc_num.vocab, open(path/'vocab_lm.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = pickle.load(open(path/'ll_lm.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pickle.load(open(path/'vocab_lm.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, bptt = 64, 70\n",
    "data = lm_databunchify(ll, bs, bptt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWD-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/3/3b/The_LSTM_cell.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, ni, nh):\n",
    "        super().__init__()\n",
    "        self.ih = nn.Linear(ni, 4*nh) #mult by 4 because internally LSTMcell splits input into 4 chunks\n",
    "        self.hh = nn.Linear(nh, 4*nh)\n",
    "        \n",
    "    def forward(self, input, state):\n",
    "        h,c = state\n",
    "        gates = (self.ih(input) + self.hh(h)).chunk(4,1)\n",
    "        ingate, forgetgate, outgate = map(torch.sigmoid, gates[:3])\n",
    "        cellgate = gates[3].tanh()\n",
    "        \n",
    "        c = (forgetgate*c) + (ingate*cellgate)\n",
    "        h = outgate * c.tanh()\n",
    "        return h, (h,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer(nn.Module):\n",
    "    def __init__(self, cell, *cell_args):\n",
    "        super().__init__()\n",
    "        self.cell = cell(*cell_args)\n",
    "        \n",
    "    def forward(self, input, state):\n",
    "        inputs = input.unbind(1)\n",
    "        outputs = []\n",
    "        for i in range(len(inputs)):\n",
    "            out, state = self.cell(inputs[i], state)\n",
    "            outputs += [out]\n",
    "        return torch.stack(outputs, dim=1), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTMLayer(LSTMCell, 300, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(64, 70, 300)\n",
    "h = (torch.zeros(64, 300), torch.zeros(64, 300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163 ms ± 307 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 y, h1 = lstm(x, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = lstm.cuda()\n",
    "x = x.cuda()\n",
    "h = (h[0].cuda(), h[1].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_fn(f):\n",
    "    f()\n",
    "    torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = partial(lstm, x, h)\n",
    "time_fn(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.3 ms ± 9.85 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 time_fn(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Builtin version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(input_size=300, hidden_size=300, num_layers=1, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(64, 70, 300)\n",
    "h = (torch.zeros(1, 64, 300), torch.zeros(1, 64, 300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 ms ± 9.28 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 y, h1 = lstm(x,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = lstm.cuda()\n",
    "x = x.cuda()\n",
    "h = (h[0].cuda(), h[1].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = partial(lstm, x, h)\n",
    "time_fn(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.46 ms ± 801 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 time_fn(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = torch.rand(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.0962e+37, 4.5811e-41, 5.0962e+37, 4.5811e-41,        nan, 4.5811e-41,\n",
       "        1.7753e+28, 2.7927e+20, 1.2723e+25, 9.8834e+17])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j.new(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def dropout_mask(x, sz, p):\n",
    "    return x.new(*sz).bernoulli_(1-p).div_(1-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 2., 2., 2., 2., 2., 0., 0., 2., 0.],\n",
       "        [0., 2., 0., 0., 0., 2., 0., 0., 2., 2.],\n",
       "        [0., 2., 2., 2., 2., 2., 2., 0., 0., 2.],\n",
       "        [2., 0., 0., 0., 2., 2., 0., 2., 0., 0.],\n",
       "        [2., 0., 2., 2., 2., 2., 0., 0., 2., 2.],\n",
       "        [2., 0., 2., 2., 2., 2., 0., 2., 0., 0.],\n",
       "        [0., 0., 0., 2., 2., 0., 2., 0., 2., 0.],\n",
       "        [2., 0., 2., 0., 2., 0., 2., 0., 0., 2.],\n",
       "        [2., 2., 0., 2., 2., 0., 0., 2., 0., 0.],\n",
       "        [2., 2., 2., 0., 0., 0., 2., 0., 0., 2.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(10, 10)\n",
    "mask = dropout_mask(x, (10, 10), 0.5); mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.4219), tensor(1.1419))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x*mask).std(), x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2,3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7222, -0.1053,  1.7117],\n",
       "         [-2.6635,  1.7559,  1.0049],\n",
       "         [ 1.9689, -0.6475,  1.3450]],\n",
       "\n",
       "        [[-0.6667, -2.4783, -0.0041],\n",
       "         [ 1.1831, -0.8572, -0.4553],\n",
       "         [ 0.4848, -0.3634, -1.3000]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.0670e+34,  3.0787e-41, -6.1824e+25]],\n",
       "\n",
       "        [[ 3.0787e-41,  8.9683e-44,  0.0000e+00]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.new(*(x.size(0), 1, x.size(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2., 2., 2.]],\n",
       "\n",
       "        [[2., 0., 2.]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jk=x.new(2,1,3).bernoulli_(0.5).div_(0.5)\n",
    "jk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.4445, -0.2105,  3.4233],\n",
       "         [-5.3269,  3.5118,  2.0099],\n",
       "         [ 3.9378, -1.2949,  2.6899]],\n",
       "\n",
       "        [[-1.3334, -0.0000, -0.0083],\n",
       "         [ 2.3662, -0.0000, -0.9107],\n",
       "         [ 0.9697, -0.0000, -2.6000]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x * jk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RNNDropout(nn.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if not self.training or self.p==0.:\n",
    "            return x\n",
    "        m = dropout_mask(x.data, (x.size(0), 1, x.size(2)), self.p)\n",
    "        return x*m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.4490,  2.1071,  0.5121, -0.2689, -0.2110,  1.7420,  0.9224],\n",
       "          [ 2.1162, -1.2908,  0.8151,  1.1954,  0.9972,  1.3228,  0.8861],\n",
       "          [ 0.8276,  0.0869, -0.9406, -0.4544,  0.1115,  0.3550, -0.4297]],\n",
       " \n",
       "         [[-0.9620,  2.0118, -0.6758,  0.2074, -1.3335, -0.7043,  0.0327],\n",
       "          [-0.0513,  1.1234,  0.7724, -0.7723,  2.4399, -1.1658, -0.7786],\n",
       "          [ 1.3043, -0.7979, -0.5535,  0.5323,  1.6160,  0.7811,  0.6359]],\n",
       " \n",
       "         [[-0.1526,  2.0579,  0.6918, -0.6077,  0.4659, -0.6876, -0.3519],\n",
       "          [ 0.9676, -0.4449, -0.8816,  1.3766,  1.5724, -0.0510, -0.8011],\n",
       "          [ 1.0664, -0.2363, -1.4780,  0.6300,  0.6918,  1.3439, -1.8077]]]),\n",
       " tensor([[[-0.6414,  3.0101,  0.7315, -0.3841, -0.0000,  2.4886,  1.3177],\n",
       "          [ 3.0232, -1.8439,  1.1645,  1.7078,  0.0000,  1.8897,  1.2658],\n",
       "          [ 1.1823,  0.1242, -1.3437, -0.6492,  0.0000,  0.5071, -0.6138]],\n",
       " \n",
       "         [[-1.3743,  0.0000, -0.9655,  0.2964, -1.9050, -1.0061,  0.0468],\n",
       "          [-0.0732,  0.0000,  1.1034, -1.1032,  3.4856, -1.6654, -1.1123],\n",
       "          [ 1.8633, -0.0000, -0.7907,  0.7605,  2.3086,  1.1158,  0.9084]],\n",
       " \n",
       "         [[-0.0000,  2.9398,  0.9883, -0.0000,  0.6656, -0.0000, -0.5027],\n",
       "          [ 0.0000, -0.6355, -1.2594,  0.0000,  2.2463, -0.0000, -1.1444],\n",
       "          [ 0.0000, -0.3375, -2.1114,  0.0000,  0.9883,  0.0000, -2.5824]]]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp = RNNDropout(0.3)\n",
    "tst_input = torch.randn(3,3,7)\n",
    "tst_input, dp(tst_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import warnings\n",
    "\n",
    "WEIGHT_HH = 'weight_hh_l0'\n",
    "\n",
    "class WeightDropout(nn.Module):\n",
    "    def __init__(self, module, weight_p=[0.], layer_names=[WEIGHT_HH]):\n",
    "        super().__init__()\n",
    "        self.module, self.weight_p, self.layer_names = module, weight_p, layer_names\n",
    "        for layer in self.layer_names:\n",
    "            w = getattr(self.module, layer)\n",
    "            self.register_parameter(f'{layer}_raw', nn.Parameter(w.data))\n",
    "            self.module._parameters[layer] = F.dropout(w, p=self.weight_p, training=False)\n",
    "            \n",
    "    def _setweights(self):\n",
    "        for layer in self.layer_names:\n",
    "            raw_w = getattr(self, f'{layer}_raw')\n",
    "            self.module._parameters[layer] = F.dropout(raw_w, p=self.weight_p, training=self.training) #applies dropout to actual weights\n",
    "    \n",
    "    def forward(self, *args):\n",
    "        self._setweights()\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            return self.module.forward(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1221, -0.0393],\n",
       "        [-0.4811, -0.5771],\n",
       "        [-0.3773, -0.2780],\n",
       "        [ 0.0526, -0.3876],\n",
       "        [ 0.3805, -0.4425],\n",
       "        [-0.3881,  0.0485],\n",
       "        [ 0.5752, -0.3762],\n",
       "        [ 0.5243,  0.2080]], requires_grad=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = nn.LSTM(5, 2)\n",
    "dp_module = WeightDropout(module, 0.4)\n",
    "getattr(dp_module.module, WEIGHT_HH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000, -0.0654],\n",
       "        [-0.8019, -0.9618],\n",
       "        [-0.6289, -0.0000],\n",
       "        [ 0.0876, -0.6460],\n",
       "        [ 0.6342, -0.0000],\n",
       "        [-0.6468,  0.0809],\n",
       "        [ 0.9587, -0.0000],\n",
       "        [ 0.8739,  0.3466]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_input = torch.randn(4, 20, 5)\n",
    "h = (torch.zeros(1, 20, 2), torch.zeros(1, 20, 2))\n",
    "x, h = dp_module(tst_input, h)\n",
    "getattr(dp_module.module, WEIGHT_HH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EmbeddingDropout(nn.Module):\n",
    "    def __init__(self, emb, embed_p):\n",
    "        super().__init__()\n",
    "        self.emb, self.embed_p = emb, embed_p\n",
    "        self.pad_idx = self.emb.padding_idx\n",
    "        if self.pad_idx is None:\n",
    "            self.pad_idx = -1\n",
    "            \n",
    "    def forward(self, words, scale=None):\n",
    "        if self.training and self.embed_p != 0:\n",
    "            size = (self.emb.weight.size(0), 1)\n",
    "            mask = dropout_mask(self.emb.weight.data, size, self.embed_p)\n",
    "            masked_embed = self.emb.weight * mask\n",
    "        else:\n",
    "            masked_embed = self.emb.weight\n",
    "        if scale:\n",
    "            masked_embed.mul_(scale)\n",
    "        return F.embedding(words, masked_embed, self.pad_idx, self.emb.max_norm,\n",
    "                          self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.8426,  3.0972, -3.1211,  0.9516, -2.4718, -0.2973,  1.6370],\n",
       "        [-0.8492,  2.2259,  3.0138, -0.0963, -3.0458, -2.7661,  0.9935],\n",
       "        [ 0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000],\n",
       "        [-1.2163,  4.1821,  4.4464, -1.2780, -1.6682, -1.5633, -0.4600],\n",
       "        [ 0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-2.8426,  3.0972, -3.1211,  0.9516, -2.4718, -0.2973,  1.6370],\n",
       "        [ 1.8341,  2.5440,  0.0676, -1.8668, -1.8180, -1.4580, -0.2391]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = nn.Embedding(100, 7, padding_idx=1)\n",
    "enc_dp = EmbeddingDropout(enc, 0.5)\n",
    "tst_input = torch.randint(0, 100, (8,))\n",
    "enc_dp(tst_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def to_detach(h):\n",
    "    \"Detaches `h` from its history\"\n",
    "    if type(h)==torch.Tensor:\n",
    "        return h.detach() \n",
    "    else:\n",
    "        return tuple(to_detach(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AWD_LSTM(nn.Module):\n",
    "    initrange=0.1\n",
    "    \n",
    "    def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token,\n",
    "                hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5):\n",
    "        super().__init__()\n",
    "        self.bs, self.emb_sz, self.n_hid, self.n_layers = 1, emb_sz, n_hid, n_layers\n",
    "        self.emb = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n",
    "        self.emb_dp = EmbeddingDropout(self.emb, embed_p)\n",
    "        self.rnns = [nn.LSTM(emb_sz if l==0 else n_hid, (n_hid if l!=n_layers-1 else emb_sz), 1,\n",
    "                            batch_first=True) for l in range(n_layers)]\n",
    "        self.rnns = nn.ModuleList([WeightDropout(rnn, weight_p) for rnn in self.rnns])\n",
    "        self.emb.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.input_dp = RNNDropout(input_p)\n",
    "        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])\n",
    "        \n",
    "    def forward(self, input):\n",
    "        bs, sl = input.size()\n",
    "        if bs!=self.bs:\n",
    "            self.bs = bs\n",
    "            self.reset()\n",
    "        raw_output = self.input_dp(self.emb_dp(input))\n",
    "        new_hidden, raw_outputs, outputs = [], [], []\n",
    "        for l, (rnn, hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n",
    "            raw_output, new_h = rnn(raw_output, self.hidden[l])\n",
    "            new_hidden.append(new_h)\n",
    "            raw_outputs.append(raw_output)\n",
    "            if l!=self.n_layers - 1:\n",
    "                raw_output = hid_dp(raw_output)\n",
    "            outputs.append(raw_output)\n",
    "        self.hidden = to_detach(new_hidden)\n",
    "        return raw_outputs, outputs\n",
    "    \n",
    "    def _one_hidden(self, l):\n",
    "        \"Return one hidden state\"\n",
    "        if l!=self.n_layers-1:\n",
    "            nh = self.n_hid\n",
    "        else:\n",
    "            nh = self.emb_sz\n",
    "        return next(self.parameters()).new(1, self.bs, nh).zero_()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"Reset the hidden states\"\n",
    "        self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LinearDecoder(nn.Module):\n",
    "    def __init__(self, n_out, n_hid, output_p, tie_encoder=None, bias=True):\n",
    "        super().__init__()\n",
    "        self.output_dp = RNNDropout(output_p)\n",
    "        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n",
    "        if bias:\n",
    "            self.decoder.bias.data.zero_()\n",
    "        if tie_encoder:\n",
    "            self.decoder.weight = tie_encoder.weight\n",
    "        else:\n",
    "            init.kaiming_uniform_(self.decoder.weight)\n",
    "            \n",
    "    def forward(self, input):\n",
    "        raw_outputs, outputs = input\n",
    "        output = self.output_dp(outputs[-1]).contiguous()\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded, raw_outputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SequentialRNN(nn.Sequential):\n",
    "    \"A sequential module that passes the reset call to its children\"\n",
    "    def reset(self):\n",
    "        for c in self.children():\n",
    "            if hasattr(c, 'reset'):\n",
    "                c.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_language_model(vocab_sz, emb_sz, n_hid, n_layers, pad_token, output_p=0.4, hidden_p=0.2, input_p=0.6,\n",
    "                      embed_p=0.1, weight_p=0.5, tie_weights=True, bias=True):\n",
    "    rnn_enc = AWD_LSTM(vocab_sz, emb_sz, n_hid=n_hid, n_layers=n_layers, pad_token=pad_token,\n",
    "                      hidden_p=hidden_p, input_p=input_p, embed_p=embed_p, weight_p=weight_p)\n",
    "    enc = rnn_enc.emb if tie_weights else None\n",
    "    return SequentialRNN(rnn_enc, LinearDecoder(vocab_sz, emb_sz, output_p, tie_encoder=enc, bias=bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_pad = vocab.index(PAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_model = get_language_model(len(vocab), 300, 300, 2, tok_pad)\n",
    "tst_model = tst_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(data.train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = tst_model(x.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded, raw_outputs, outputs = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4480, 60003])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_outputs), len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([torch.Size([64, 70, 300]), torch.Size([64, 70, 300])],\n",
       " [torch.Size([64, 70, 300]), torch.Size([64, 70, 300])])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[o.size() for o in raw_outputs], [o.size() for o in outputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GradientClipping(Callback):\n",
    "    def __init__(self, clip=None):\n",
    "        self.clip = clip\n",
    "    def after_backward(self):\n",
    "        if self.clip:\n",
    "            nn.utils.clip_grad_norm_(self.run.model.parameters(), self.clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RNNTrainer(Callback):\n",
    "    def __init__(self, alpha, beta):\n",
    "        self.alpha, self.beta = alpha, beta\n",
    "        \n",
    "    def after_pred(self):\n",
    "        # save the extra outputs for later and only returns the true output\n",
    "        self.raw_out, self.out = self.pred[1], self.pred[2]\n",
    "        self.run.pred = self.pred[0]\n",
    "        \n",
    "    def after_loss(self):\n",
    "        #AR and TAR\n",
    "        if self.alpha != 0.:\n",
    "            self.run.loss += self.alpha * self.out[-1].float().pow(2).mean()\n",
    "        if self.beta != 0.:\n",
    "            h = self.raw_out[-1]\n",
    "            if len(h)>1:\n",
    "                self.run.loss += self.beta * (h[:,1:] - h[:, :-1]).float().pow(2).mean()\n",
    "    \n",
    "    def begin_epoch(self):\n",
    "        if hasattr(self.dl.dataset, \"batchify\"):\n",
    "            self.dl.dataset.batchify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cross_entropy_flat(input, target):\n",
    "    bs, sl = target.size()\n",
    "    return F.cross_entropy(input.view(bs*sl, -1), target.view(bs*sl))\n",
    "\n",
    "def accuracy_flat(input, target):\n",
    "    bs, sl = target.size()\n",
    "    return accuracy(input.view(bs*sl, -1), target.view(bs*sl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_sz, nh, nl = 300, 300, 2\n",
    "model = get_language_model(len(vocab), emb_sz, nh, nl, tok_pad, input_p=0.6, output_p=0.4, weight_p=0.5,\n",
    "                          embed_p=0.1, hidden_p=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [partial(AvgStatsCallback, accuracy_flat),\n",
    "      CudaCallback, Recorder,\n",
    "      partial(GradientClipping, clip=0.1),\n",
    "      partial(RNNTrainer, alpha=2., beta=1.),\n",
    "      ProgressCallback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(model, data, cross_entropy_flat, lr=5e-3, cb_funcs=cbs, opt_func=adam_opt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy_flat</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_accuracy_flat</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.141464</td>\n",
       "      <td>0.197553</td>\n",
       "      <td>4.608424</td>\n",
       "      <td>0.240792</td>\n",
       "      <td>46:43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted 12a_awd_lstm.ipynb to nb_12a.py\r\n"
     ]
    }
   ],
   "source": [
    "!python notebook2script.py 12a_awd_lstm.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_learnai_env)",
   "language": "python",
   "name": "conda_learnai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
